{"name":"Briareus","tagline":"Accelerating Python Applications using Cloud","body":"\r\n**This is only a prototype project and it is not suitable for production environments!**\r\n\r\n**This document is to show you what Briareus can do, but not how to use it. If you are going to use Briareus, please contact me (zhaomeng.zhu@gmail.com) for more details. Thank you!**\r\n\r\n# About\r\n\r\nBriareus aims to speed up python applications using distributed platforms like Cloud. It can automatically parallelize loops (including the `for` loops, list comprehensions and the `map` function), making functions asynchronous, or migrate functions to be evaluated in remote servers. To achieve these goals, only minimal modifications of the source code is needed.\r\n\r\nThis repo only contains the code related to the interfaces and code transformations. The distributed framework used and the task queue are in [Corellia](https://github.com/Tefx/Corellia), and the serialization part is in [Husky](https://github.com/Tefx/Husky).\r\n\r\n# Installation and deployment\r\n\r\nPlease contact me via zhaomeng.zhu@gmail.com\r\n\r\n# Features\r\n\r\nTo use Briareus, a patch in the first line of the source file (the `__main__` file) is required:\r\n\r\n    from Briareus import patch; patch()\r\n\r\nThis monkey patch will release the power of Briareus!\r\n\r\nThere are three operations provided by Briareus, and all of them are enabled by comments:\r\n\r\n    # remote\r\n    # async\r\n    # parallelize\r\n\r\n*Why we use comments?*\r\n\r\n*Because by this way, the behavior of the program will not be changed if `patch()` is not applied, or if the target platforms is not available.*\r\n\r\n\r\nThe `# remote` makes a following function's evaluations be migrated to remote servers. For example,\r\n\r\n    from Briareus import patch; patch()\r\n\r\n    # remote\r\n    def foo(a, b):\r\n      return a+b\r\n\r\n    print \"1+2=%d\" % foo(1,2)\r\n\r\nHere, the evaluation of `1+2` will be calculated in a pre-configured remote server. However, the other part of the program still run locally.\r\n\r\nThe `# async` makes a following function asynchronous. For example,\r\n\r\n    from Briareus import patch; patch()\r\n\r\n    # async\r\n    def foo1(...):\r\n        ...\r\n        ...\r\n\r\n    a = foo1(...)\r\n    b = foo1(...)\r\n\r\n    bar(a, b)\r\n\r\n\r\nHere, the evaluation of `b` starts without waiting for the finish of the evaluation of `a`. However, `bar(a,b)` will not start until both the evaluations of `a` and `b` has finished.\r\n\r\nOf course, this comment can be used together with `# remote`:\r\n\r\n    from Briareus import patch; patch()\r\n\r\n    # async\r\n    # remote\r\n    def foo1(...):\r\n        ...\r\n\r\n    # async\r\n    # remote\r\n    def foo2(...):\r\n        ...\r\n\r\n    a = foo1(...)\r\n    b = foo2(...)\r\n\r\n    bar(a, b)\r\n\r\nNow, `a` and `b` is evaluated simultaneously in the configured distributed environment!\r\n\r\nFinally, `# parallelize` parallelizes a following `for` loop, `map` invocation and *list comprehension*:\r\n\r\n    from Briareus import patch; patch()\r\n\r\n    # paralleliz\r\n    for a in l:\r\n        do_something(a)\r\n\r\n    # paralleliz\r\n    for a,b,c in l:\r\n        do_something(a)\r\n        do_other_thing(b,c)\r\n\r\n    # paralleliz\r\n    for a in l0:\r\n        for b in l1:\r\n            for c in l2:\r\n                do_something(a,b,c)\r\n\r\n    # parallelize\r\n    after = map(foo, l)\r\n\r\n    # parallelize\r\n    new_list = [x*2 for x in l if x > 0]\r\n\r\n    # parallelize\r\n    new_list2 = [x*y+z for x in l0 if x>0 \\\r\n                       for y in l1 if y>0 \\\r\n                       for z in l3]\r\n\r\nAll of the above loops are parallelized!\r\n\r\n\r\nNow, combine the use of `# remote` and `# parallelize` in a real-world example implementing the OMP algorithm:\r\n\r\n    from Briareus import patch; patch()\r\n    import numpy as np\r\n    from scipy\r\n    import sparse\r\n\r\n    # remote\r\n    def OMP(s, T, N):\r\n        body_of_OMP\r\n\r\n    def recovery_image(a, b, Y, R, ww):\r\n        X = np.zeros((a, b))\r\n\r\n        # parallelize with const R\r\n        for i in xrange(b):\r\n            X[:,i] = OMP(Y[:,i].reshape((-1,1)), R, a)\r\n            X1 = ww.H * sparse.csr_matrix(X) * ww\r\n\r\n        return X1.toarray()\r\n\r\n    if __name__ == \"__main__\":\r\n        a, b, Y, R, ww, original =\r\n        perpare_image()\r\n        recovered = recovery_image(a, b, Y, R, ww, original)\r\n        errorx = (np.absolute(recovered - original) ** 2).sum()\r\n        psnr = 10 * np.log10(255 * 255 / (errorx / a / b))\r\n\r\n        return psnr\r\n\r\nGreat! The algorithm has been parallelized in a distributed environment!\r\n\r\nYou may notice that here we use a slightly different comment `# paralleliz with const R`. This comment distributes and caches the large variable `R` in distributed workers. Of course, there can be more than one cached variables:\r\n\r\n    # parallelize with const a\r\n\r\n    # parallelize with const a, b, c\r\n\r\n    # parallelize with const a, b and c\r\n\r\nor, if you like,\r\n\r\n    # parallelize with cached a\r\n\r\n    # parallelize with cached a, b, c\r\n\r\n    # parallelize with cached a, b and c\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}